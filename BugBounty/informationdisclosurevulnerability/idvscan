1(Discovering Login credential database) - Robots.txt dosyasını incele. -> Robots.txt içerisinde botların indekslenmesini engellemiş olduğu adresleri tespit et. Genellikle backup adres erişimi gibi yerleri tespit edebiliriz. 
2(Sensitive data(hassas veri) and discovering endpoints) - Feroxbuster aracını kullanarak pathlerin tespiti için bruteforce yaparak tespit edebiliriz. Wordlist kullanımı için Seclist kullanılabilir.--> https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-Content/common.txt -> 
3(Admin login information) - Elde edilen credentiallar ile nerelere authetice olunabiliyor ona bakmalısın. -> ./git -> git dosyalarını bulursan git indirerek dosyaları git üzerinde çalıştır. 
4 - GET-POST parametreleri üzerinde oynamalar yaparak sunucunun verdiği tepkilere göre üretilen bir hata mesajı varsa sunucu hakkında bilgi sızdırıyor ise bunu araştırmalı ve rapora eklemeliyiz.





                                                      STRUCTURE OF URL
                             -- https://www.example.com:80/file.html?key1=value#20asa3r21 --

https://    www.               example.com         :80                  /file.html         ?            key1='value'     #20asa3r21
--------    ---                -----------         ----                 ----------        ---           -----------      --------
Protocol    Sub Domain         Domain Name         Port                    Path           query         Parameters       fragment


